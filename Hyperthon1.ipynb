!pip install pydub
!pip install torch torchvision torchaudio
!pip install transformers
!pip install openai-whisper
!pip install webrtcvad
!pip install noisereduce
!pip install srt
!pip install librosa
!apt-get install ffmpeg
!pip install scipy

from google.colab import drive
drive.mount('/content/drive')

import os
from pydub import AudioSegment
from transformers import M2M100ForConditionalGeneration, M2M100Tokenizer
import whisper
import srt
from datetime import timedelta
import webrtcvad
import wave

def preprocess_audio_for_vad(input_audio_path, output_audio_path):
    """
    Preprocess audio for WebRTC VAD:
    - Convert to mono, 16-bit PCM, 16 kHz WAV format
    - Extract speech using WebRTC VAD
    """
    print("Preprocessing audio for VAD...")

    # Load and convert audio to mono and 16 kHz
    audio = AudioSegment.from_file(input_audio_path)
    audio = audio.set_channels(1)  # Mono
    audio = audio.set_frame_rate(16000)  # 16 kHz

    # Export audio to a temporary WAV file
    temp_wav_path = "temp_audio.wav"
    audio.export(temp_wav_path, format="wav")

    # Read the WAV file and process it with WebRTC VAD
    vad = webrtcvad.Vad()
    vad.set_mode(2)  # Balanced mode for speech detection

    with wave.open(temp_wav_path, "rb") as wf:
        # Check audio format
        assert wf.getnchannels() == 1, "Audio must be mono"
        assert wf.getsampwidth() == 2, "Audio must be 16-bit PCM"
        assert wf.getframerate() == 16000, "Audio must have 16 kHz sample rate"

        frame_duration = 30  # ms (10, 20, or 30 ms frames supported)
        frame_size = int(wf.getframerate() * (frame_duration / 1000)) * wf.getsampwidth()

        speech_frames = []
        while True:
            frames = wf.readframes(frame_size // wf.getsampwidth())
            if len(frames) < frame_size:
                break  # End of file

            # Use WebRTC VAD to detect speech
            is_speech = vad.is_speech(frames, wf.getframerate())
            if is_speech:
                speech_frames.append(frames)

    # Combine speech frames into a new audio file
    with wave.open(output_audio_path, "wb") as out_wf:
        out_wf.setnchannels(1)
        out_wf.setsampwidth(2)
        out_wf.setframerate(16000)
        out_wf.writeframes(b"".join(speech_frames))

    print(f"Preprocessed audio saved to: {output_audio_path}")


# Transcribe audio to text using Whisper
def transcribe_audio_whisper(audio_path, whisper_model):
    print("Transcribing audio...")
    result = whisper_model.transcribe(audio_path, language="ja")
    print("Transcription completed.")
    return result["segments"]


# Translate text using M2M-100
def translate_text_m2m100(text, m2m_model, m2m_tokenizer, src_lang="ja", tgt_lang="en"):
    m2m_tokenizer.src_lang = src_lang
    encoded = m2m_tokenizer(text, return_tensors="pt")
    generated_tokens = m2m_model.generate(
        **encoded,
        forced_bos_token_id=m2m_tokenizer.lang_code_to_id[tgt_lang],
        max_length=512
    )
    return m2m_tokenizer.decode(generated_tokens[0], skip_special_tokens=True)


# Create an SRT file from transcription and translation
def create_srt(transcriptions, model, tokenizer):
    subtitles = []
    for segment in transcriptions:
        start = timedelta(seconds=segment["start"])
        end = timedelta(seconds=segment["end"])
        japanese_text = segment["text"]
        english_translation = translate_text_m2m100(japanese_text, model, tokenizer)

        subtitle = srt.Subtitle(
            index=len(subtitles) + 1,
            start=start,
            end=end,
            content=f"{english_translation}\n({japanese_text})",
        )
        subtitles.append(subtitle)
    return subtitles


# Main function
def main(input_video_path, output_srt_path, temp_audio_path="temp_audio.wav"):
    # Load Whisper and M2M-100 models
    print("Loading models...")
    whisper_model = whisper.load_model("large")  # Use Whisper large model for better accuracy
    m2m_model_name = "facebook/m2m100_418M"
    m2m_tokenizer = M2M100Tokenizer.from_pretrained(m2m_model_name)
    m2m_model = M2M100ForConditionalGeneration.from_pretrained(m2m_model_name)
    print("Models loaded successfully.")

    # Preprocess audio
    preprocess_audio_for_vad(input_video_path, temp_audio_path)

    # Transcribe audio
    transcriptions = transcribe_audio_whisper(temp_audio_path, whisper_model)

    # Create subtitles
    print("Creating SRT file...")
    subtitles = create_srt(transcriptions, m2m_model, m2m_tokenizer)
    with open(output_srt_path, "w", encoding="utf-8") as f:
        f.write(srt.compose(subtitles))
    print(f"Subtitle file created: {output_srt_path}")

    # Cleanup temporary files
    if os.path.exists(temp_audio_path):
        os.remove(temp_audio_path)


# Run the pipeline
if __name__ == "__main__":
    VIDEO_PATH = "/content/drive/MyDrive/Hyperthon/vid.mp4"  # Replace with your actual file path
    OUTPUT_SRT_PATH = "/content/drive/MyDrive/Hyperthon/outputNew1.srt"  # Replace with the desired output path

    main(VIDEO_PATH, OUTPUT_SRT_PATH)
